{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset is designed for machine learning classification tasks and contains in total 60 000 training and 10 000 test images (gray scale) with each 28x28 pixel. Each training and test case is associated with one of ten labels (0–9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages and functions and set the session seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, SpatialDropout2D\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST data from Keras\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data by dividing through the maximum pixel value (=255)\n",
    "train_images = train_images / train_images.max()\n",
    "test_images = test_images / test_images.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building a simple neural network containing one hidden layer. Note that as here we use the untransformed but normalized data, we need to flatten the 28 by 28 pixels input first. We add one hidden densely-connected layer which performs output = relu(dot(input, kernel) + bias), where the rectified linear unit (relu) activation function has been proven to work well. We set the number of nodes equal to 128, because this seems to work well in our case. The number of nodes could essentially be any of the numbers 32, 64, 128, 256 and 512, as these are in a sequence of multiples between the number of nodes in the output (= 10) and input (= 784) layers. The softmax layer then assigns predicted probabilities to each of the ten clothing categories, which is also why there are ten nodes in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Build a simple three-layer (1 hidden layer) model\n",
    "# The input size is 28 x 28 pixels and is flattened to a vector of length 784\n",
    "# The activation function is RELU (rectified linear unit) and performs the \n",
    "# multiplication of input and weights (plus bias)\n",
    "# The output (softmax) layer returns probabilities for all ten classes\n",
    "three_layer_model = Sequential()\n",
    "three_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "three_layer_model.add(Dense(128, activation = 'relu'))\n",
    "three_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the neural network, we compile it. We specify sparse_categorical_crossentropy as the loss function, which is suitable for categorical multi-class responses. The optimizer controls the learning rate; adam (adaptive moment estimation) is similar to classical stochastic gradient descent and usually a safe choice for the optimizer. We set our metric of interest to be the accuracy, or the percentage of correctly classified images. Hereafter, we fit the model onto our training data set using ten iterations through the training data (“epochs”). Here, 70% is used for training and 30% is used for validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.5336 - accuracy: 0.8143 - val_loss: 0.4403 - val_accuracy: 0.8492\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.3992 - accuracy: 0.8575 - val_loss: 0.3984 - val_accuracy: 0.8622\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.3572 - accuracy: 0.8713 - val_loss: 0.3596 - val_accuracy: 0.8677\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.3285 - accuracy: 0.8795 - val_loss: 0.3622 - val_accuracy: 0.8682\n",
      "Epoch 5/10\n",
      " - 10s - loss: 0.3070 - accuracy: 0.8873 - val_loss: 0.3440 - val_accuracy: 0.8759\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.2927 - accuracy: 0.8924 - val_loss: 0.3270 - val_accuracy: 0.8839\n",
      "Epoch 7/10\n",
      " - 10s - loss: 0.2784 - accuracy: 0.8975 - val_loss: 0.3323 - val_accuracy: 0.8822\n",
      "Epoch 8/10\n",
      " - 10s - loss: 0.2660 - accuracy: 0.9003 - val_loss: 0.3339 - val_accuracy: 0.8839\n",
      "Epoch 9/10\n",
      " - 10s - loss: 0.2572 - accuracy: 0.9042 - val_loss: 0.3477 - val_accuracy: 0.8786\n",
      "Epoch 10/10\n",
      " - 10s - loss: 0.2479 - accuracy: 0.9069 - val_loss: 0.3478 - val_accuracy: 0.8759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1da3b35b4c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Sparse categorical cross-entropy is the loss function for integer labels\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "three_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                          optimizer = 'adam', metrics = ['accuracy'])\n",
    "three_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                      validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we print the results of the model in terms of training and testing loss and accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Model with three layers and ten epochs -- Test loss: 37.564722998142244\n",
      "Model with three layers and ten epochs -- Test accuracy: 87.09\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = three_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with three layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with three layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the neural network with one hidden layer already performs relatively well with a test accuracy of 87.09%. However, it seems like we are slightly overfitting (i.e. the model is fitted too well to a particular data set and therefore does not well extend to other data sets), as the training set accuracy (88.15%) is slightly higher than the test set accuracy. There are several ways to avoid overfitting in neural networks, such as simplifying our model by reducing the number of hidden layers and neurons, adding dropout layers that randomly remove some of the connections between layers, and early stopping when validation loss starts to increase. Later on in this post, I will demonstrate some of these methods to you. For further reading, I personally like this and this post showing how to avoid overfitting when building neural networks using keras. Instead, to see whether a deep neural network performs better at predicting clothing categories, we build a neural network with three hidden layers in a similar way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly as before, build a five-layer (3 hidden layers) model\n",
    "five_layer_model = Sequential()\n",
    "five_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(128, activation = 'relu'))\n",
    "five_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.5244 - acc: 0.8120 - val_loss: 0.4311 - val_acc: 0.8476\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.3799 - acc: 0.8594 - val_loss: 0.3547 - val_acc: 0.8723\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.3435 - acc: 0.8731 - val_loss: 0.3834 - val_acc: 0.8597\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.3204 - acc: 0.8816 - val_loss: 0.4051 - val_acc: 0.8595\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.3008 - acc: 0.8876 - val_loss: 0.3801 - val_acc: 0.8619\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.2855 - acc: 0.8920 - val_loss: 0.3164 - val_acc: 0.8858\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.2716 - acc: 0.8976 - val_loss: 0.3603 - val_acc: 0.8739\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.2613 - acc: 0.9010 - val_loss: 0.3277 - val_acc: 0.8859\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.2478 - acc: 0.9065 - val_loss: 0.3611 - val_acc: 0.8738\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.2426 - acc: 0.9096 - val_loss: 0.3602 - val_acc: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b36d956bc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "five_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                         optimizer = 'adam', metrics = ['accuracy'])\n",
    "five_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                     validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n",
      "Model with five layers and ten epochs -- Test loss: 38.866063070297244\n",
      "Model with five layers and ten epochs -- Test accuracy: 86.4\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = five_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with five layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with five layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the model with two additional layers does not perform better than the previous one with only one hidden layer, given that both the training (87.42%) and test set (86.03%) accuracies are lower and the loss (38.49) is higher. Let’s try whether adding another five hidden layers improves model performance, or whether we can include that increasing model complexity does not improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly as before, build a ten-layer (8 hidden layers) model\n",
    "ten_layer_model = Sequential()\n",
    "ten_layer_model.add(Flatten(input_shape = (28, 28)))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(128, activation = 'relu'))\n",
    "ten_layer_model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.5908 - acc: 0.7852 - val_loss: 0.4850 - val_acc: 0.8327\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.4296 - acc: 0.8448 - val_loss: 0.4259 - val_acc: 0.8468\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.3886 - acc: 0.8616 - val_loss: 0.4002 - val_acc: 0.8511\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.3638 - acc: 0.8689 - val_loss: 0.4008 - val_acc: 0.8636\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.3470 - acc: 0.8749 - val_loss: 0.3682 - val_acc: 0.8678\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.3259 - acc: 0.8809 - val_loss: 0.3866 - val_acc: 0.8661\n",
      "Epoch 7/10\n",
      " - 10s - loss: 0.3130 - acc: 0.8876 - val_loss: 0.3562 - val_acc: 0.8729\n",
      "Epoch 8/10\n",
      " - 10s - loss: 0.3056 - acc: 0.8889 - val_loss: 0.3682 - val_acc: 0.8696\n",
      "Epoch 9/10\n",
      " - 10s - loss: 0.2922 - acc: 0.8935 - val_loss: 0.3439 - val_acc: 0.8809\n",
      "Epoch 10/10\n",
      " - 10s - loss: 0.2793 - acc: 0.8992 - val_loss: 0.3399 - val_acc: 0.8829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b36ddb8108>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 10 epochs\n",
    "ten_layer_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                        optimizer = 'adam', metrics = ['accuracy'])\n",
    "ten_layer_model.fit(train_images, train_labels, epochs = 10, \n",
    "                    validation_split = 0.3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 83us/step\n",
      "Model with ten layers and ten epochs -- Test loss: 36.5664215695858\n",
      "Model with ten layers and ten epochs -- Test accuracy: 87.61\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = ten_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with ten layers and ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with ten layers and ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with eight hidden layers performs best in terms of training (88.21%) and test (87.58%) accuracy as well as loss (36.12). Nevertheless, the difference in performance between the first model with one hidden layer and the current model with eight hidden layers is only quite small. Although it seems that with so many hidden layers, we can model additional complexity that improves the accuracy of the model, we must ask ourselves whether increasing model complexity at the cost of interpretability and computational feasibility is worth this slight improvement in accuracy and loss.\n",
    "\n",
    "Now that we have seen how the number of hidden layers affects model performance, let’s try and see whether increasing the number of epochs (i.e. the number of times the model iterates through the training data) from ten to fifty improves the performance of our first neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 0.2355 - acc: 0.9119 - val_loss: 0.3279 - val_acc: 0.8823\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.2266 - acc: 0.9168 - val_loss: 0.3180 - val_acc: 0.8879\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.2194 - acc: 0.9187 - val_loss: 0.3233 - val_acc: 0.8863\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.2109 - acc: 0.9218 - val_loss: 0.3260 - val_acc: 0.8878\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.2062 - acc: 0.9231 - val_loss: 0.3592 - val_acc: 0.8770\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.1960 - acc: 0.9272 - val_loss: 0.3394 - val_acc: 0.8832\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.1919 - acc: 0.9279 - val_loss: 0.3382 - val_acc: 0.8907\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.1875 - acc: 0.9299 - val_loss: 0.3392 - val_acc: 0.8894\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.1801 - acc: 0.9330 - val_loss: 0.3422 - val_acc: 0.8900\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.1758 - acc: 0.9332 - val_loss: 0.3469 - val_acc: 0.8892\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.1710 - acc: 0.9366 - val_loss: 0.3454 - val_acc: 0.8892\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.1644 - acc: 0.9388 - val_loss: 0.3825 - val_acc: 0.8784\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.1623 - acc: 0.9393 - val_loss: 0.3466 - val_acc: 0.8912\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.1557 - acc: 0.9419 - val_loss: 0.3643 - val_acc: 0.8873\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.1541 - acc: 0.9417 - val_loss: 0.3538 - val_acc: 0.8896\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.1486 - acc: 0.9465 - val_loss: 0.3764 - val_acc: 0.8844\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.1458 - acc: 0.9457 - val_loss: 0.3713 - val_acc: 0.8927\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.1413 - acc: 0.9475 - val_loss: 0.3638 - val_acc: 0.8937\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.1378 - acc: 0.9493 - val_loss: 0.3907 - val_acc: 0.8901\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.1358 - acc: 0.9496 - val_loss: 0.3865 - val_acc: 0.8904\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.1325 - acc: 0.9516 - val_loss: 0.3767 - val_acc: 0.8931\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.1293 - acc: 0.9521 - val_loss: 0.4187 - val_acc: 0.8871\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.1248 - acc: 0.9531 - val_loss: 0.3985 - val_acc: 0.8906\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.1203 - acc: 0.9552 - val_loss: 0.4317 - val_acc: 0.8867\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.1185 - acc: 0.9558 - val_loss: 0.4073 - val_acc: 0.8891\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.1179 - acc: 0.9569 - val_loss: 0.4280 - val_acc: 0.8833\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.1149 - acc: 0.9570 - val_loss: 0.4469 - val_acc: 0.8866\n",
      "Epoch 28/50\n",
      " - 7s - loss: 0.1115 - acc: 0.9592 - val_loss: 0.4181 - val_acc: 0.8906\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.1069 - acc: 0.9604 - val_loss: 0.4451 - val_acc: 0.8876\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.1075 - acc: 0.9599 - val_loss: 0.4492 - val_acc: 0.8865\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.1054 - acc: 0.9611 - val_loss: 0.4266 - val_acc: 0.8929\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.1028 - acc: 0.9625 - val_loss: 0.4386 - val_acc: 0.8878\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.0975 - acc: 0.9646 - val_loss: 0.4433 - val_acc: 0.8917\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.1028 - acc: 0.9615 - val_loss: 0.4612 - val_acc: 0.8859\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.0973 - acc: 0.9646 - val_loss: 0.4601 - val_acc: 0.8911\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.0944 - acc: 0.9661 - val_loss: 0.4863 - val_acc: 0.8884\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.0933 - acc: 0.9653 - val_loss: 0.4684 - val_acc: 0.8866\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.0876 - acc: 0.9680 - val_loss: 0.4982 - val_acc: 0.8823\n",
      "Epoch 39/50\n",
      " - 6s - loss: 0.0882 - acc: 0.9680 - val_loss: 0.5246 - val_acc: 0.8857\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.0876 - acc: 0.9665 - val_loss: 0.4885 - val_acc: 0.8869\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.0833 - acc: 0.9689 - val_loss: 0.4746 - val_acc: 0.8896\n",
      "Epoch 42/50\n",
      " - 6s - loss: 0.0834 - acc: 0.9698 - val_loss: 0.4960 - val_acc: 0.8879\n",
      "Epoch 43/50\n",
      " - 6s - loss: 0.0807 - acc: 0.9702 - val_loss: 0.4801 - val_acc: 0.8921\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.0829 - acc: 0.9694 - val_loss: 0.5105 - val_acc: 0.8888\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.0777 - acc: 0.9714 - val_loss: 0.5050 - val_acc: 0.8923\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.0747 - acc: 0.9721 - val_loss: 0.5333 - val_acc: 0.8868\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.0779 - acc: 0.9716 - val_loss: 0.5201 - val_acc: 0.8897\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.0755 - acc: 0.9733 - val_loss: 0.5678 - val_acc: 0.8830\n",
      "Epoch 49/50\n",
      " - 6s - loss: 0.0734 - acc: 0.9727 - val_loss: 0.5271 - val_acc: 0.8846\n",
      "Epoch 50/50\n",
      " - 6s - loss: 0.0718 - acc: 0.9736 - val_loss: 0.5305 - val_acc: 0.8881\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with accuracy metric and adam optimizer\n",
    "# Fit the model using 70 percent of the data and 50 epochs\n",
    "three_layer_model_50_epochs = three_layer_model.fit(train_images, train_labels, \n",
    "                                                  epochs = 50, validation_split = 0.3,\n",
    "                                                  verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Model with three layers and fifty epochs -- Test loss: 57.36023781448603\n",
      "Model with three layers and fifty epochs -- Test accuracy: 87.99\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = three_layer_model.evaluate(test_images, test_labels)\n",
    "print(\"Model with three layers and fifty epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Model with three layers and fifty epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three-layer model trained with fifty epochs has the highest train (89.32%) and test (88.68%) accuracies we have seen so far. However, the loss (54.73) is also about a third larger than we have seen before. Additionally, the model is also less time-efficient, given that the increase in accuracy is not substantial but the model takes significantly longer to fit. To better understand the trade-off between minimizing loss and maximizing accuracy, we plot model loss and accuracy over the number of epochs for the training and cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss as function of epochs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(three_layer_model_50_epochs.history['val_loss'], 'blue')\n",
    "plt.plot(three_layer_model_50_epochs.history['loss'], 'red')\n",
    "plt.legend(['Cross-validation', 'Training'], loc = 'upper left')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "# Plot accuracy as function of epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(three_layer_model_50_epochs.history['val_acc'], 'blue')\n",
    "plt.plot(three_layer_model_50_epochs.history['acc'], 'red')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplots_adjust(wspace = .35)\n",
    "\n",
    "# Include plot title and show the plot\n",
    "plt.suptitle('Model loss and accuracy over epochs for a three-layer neural network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for the training data, loss decreases to zero while accuracy increases to one, as a result of overfitting. This is why we also check how the model performs on the cross-validation data, for which we observe that loss increases with the number of epochs while accuracy remains relatively stable. Using this figure, we can select an “optimal” number of epochs such that accuracy is maximized while loss is minimized. Looking at the cross-validation data accuracy, we see that the accuracy peak lays at around 20 epochs, for which loss is approximately 0.4. However, similar accuracies but much lower losses and modelling time are achieved with around 6 and 12 epochs, and so we might rather choose to train our model with around 6 or 20 epochs.\n",
    "\n",
    "Regarding the model output, the predictions returned are probabilities per class or clothing category. We can calculate the majority vote by taking class that has the maximum of predicted probabilities of all classes. We can print the first ten elements of the majority_vote dictionary, which we can obtain as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0: 9\n",
      "Actual 0: 9\n",
      "Prediction 1: 2\n",
      "Actual 1: 2\n",
      "Prediction 2: 1\n",
      "Actual 2: 1\n",
      "Prediction 3: 1\n",
      "Actual 3: 1\n",
      "Prediction 4: 6\n",
      "Actual 4: 6\n",
      "Prediction 5: 1\n",
      "Actual 5: 1\n",
      "Prediction 6: 4\n",
      "Actual 6: 4\n",
      "Prediction 7: 6\n",
      "Actual 7: 6\n",
      "Prediction 8: 5\n",
      "Actual 8: 5\n",
      "Prediction 9: 7\n",
      "Actual 9: 7\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print predictions versus actual labels\n",
    "predictions = three_layer_model.predict(test_images)\n",
    "for i in range(10):\n",
    "  print(\"Prediction \" + str(i) + \": \" + str(np.argmax(np.round(predictions[i]))))\n",
    "  print(\"Actual \" + str(i) + \": \" + str(test_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All except the fifth (number 4) prediction are correct. In the fifth prediction, a shirt (category 6) is being misclassified as a top (category 0).\n",
    "\n",
    "# Convolutional Neural Network\n",
    "I also wanted to show you how to build a convolutional neural network and compare its performance to the neural networks presented earlier, mostly because convolutional neural networks have generally been shown to perform better on visual image data. Essentially, what happens in a convolutional neural network is that a smaller matrix (the “filter matrix” or “kernel”) slides over the full image matrix, moving pixel by pixel, multiplies the filter matrix with the part of the full image matrix covered by the filter matrix on that moment, sums up these values and then repeats this until the full image matrix has been covered. For a more extensive explanation on how convolutional neural networks, I refer you to this page or this page.\n",
    "\n",
    "As we need to prepare our data slightly differently for a convolutional neural network, we reload the data and reshape the images to “flatten” them. The last “1” in the reshape dimensions stand for a greyscale, as we have images on a black-to-white scale. If we would have RGB images, we would change the “1” into a “3”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data for a convolutional neural network\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure the the values of the pixels, ranging from zero to 255, are of the float type and then we normalize the values as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to the correct format (the last 1 stands for greyscale)\n",
    "train_images = train_images.reshape(60000, 28, 28, 1)\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image data to numeric data and normalize them\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / train_images.max()\n",
    "test_images = test_images / test_images.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional neural network cannot deal with categorical labels. Therefore, we transform the labels to binary vectors, where all vectors have length ten (as there are ten categories), a “1” at the index of the category and zeros elsewhere. For example, category 3 and 8 would be coded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], respectively. This transformation is referred to as “one hot encoding”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the label data\n",
    "# Convert every number to a vector of the length of the number of categories\n",
    "# The vector has zero everywhere except a one on the position of the number it \n",
    "# represents. Example: 3 = [0 0 0 1 0 0 0 0 0 0]\n",
    "train_labels_bin = to_categorical(train_labels)\n",
    "test_labels_bin = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start building our convolutional neural network. The first layer Conv2D is a convolutional layer that takes a 2-dimensional matrix of 28 by 28 pixels in greyscale (1) as input. As before, we use 128 nodes in this layer, as the size of the data is not extremely large and we want to avoid making our model unnecessarily complex. The filter matrix is of size 3 by 3, which is quite standard. As before, we use the rectified linear (“relu”) activation function. The MaxPooling2D layer reduces the dimensionality (and thus required computational power) by outputting the maximum of the part of the input image that is captured by the filter matrix. The Flatten layer simply flattens the result from the previous layer into a vector. As we saw before, the softmax layer then assigns predicted probabilities to each of the ten clothing categories. Note that we use the same optimizer and metric as before, but that we now use “categorical_crossentropy” as the loss function instead of “sparse_categorical_crossentropy”. The reason for this is that the former works for one-hot encoded labels, whereas the other works for categorical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a convolutional neural network with two convolutional layers\n",
    "conv_model = Sequential()\n",
    "conv_model.add(Conv2D(128, (3, 3), input_shape = (28, 28, 1)))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model.add(Conv2D(128, (3, 3)))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(128))\n",
    "conv_model.add(Dense(10))\n",
    "conv_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our model to the training data, where we set the batch_size argument equal to the number of neurons in the convolutional layers (= 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 209s - loss: 0.4639 - acc: 0.8333\n",
      "Epoch 2/10\n",
      " - 208s - loss: 0.3025 - acc: 0.8910\n",
      "Epoch 3/10\n",
      " - 206s - loss: 0.2644 - acc: 0.9041\n",
      "Epoch 4/10\n",
      " - 206s - loss: 0.2345 - acc: 0.9148\n",
      "Epoch 5/10\n",
      " - 206s - loss: 0.2153 - acc: 0.9216\n",
      "Epoch 6/10\n",
      " - 207s - loss: 0.1937 - acc: 0.9295\n",
      "Epoch 7/10\n",
      " - 207s - loss: 0.1772 - acc: 0.9353\n",
      "Epoch 8/10\n",
      " - 208s - loss: 0.1599 - acc: 0.9425\n",
      "Epoch 9/10\n",
      " - 212s - loss: 0.1453 - acc: 0.9482\n",
      "Epoch 10/10\n",
      " - 204s - loss: 0.1328 - acc: 0.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b36e917688>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model with adam optimizer and accuracy metric\n",
    "# Categorical cross-entropy is the loss function for one-hot encoded labels and\n",
    "# batch size equal to the number of neurons in the convolutional layers and 10 epochs\n",
    "conv_model.compile(loss = \"categorical_crossentropy\", \n",
    "                   optimizer = 'adam', metrics = ['accuracy'])\n",
    "conv_model.fit(train_images, train_labels_bin, batch_size = 128, \n",
    "               epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "Convolutional model ten epochs -- Test loss: 29.167398313879968\n",
      "Convolutional model ten epochs -- Test accuracy: 90.7\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = conv_model.evaluate(test_images, test_labels_bin)\n",
    "print(\"Convolutional model ten epochs -- Test loss:\", test_loss * 100)\n",
    "print(\"Convolutional model ten epochs -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are still overfitting, we observe that the convolutional neural network performs better than the neural networks we saw earlier, achieving a training set accuracy of 95.16% and a test set accuracy of 90.39%, and a lower loss of 28.70. This was to be expected, because convolutional neural networks have previously been shown to perform well on visual imagery data. Let’s see if we can reduce overfitting by reducing the number of neurons from 128 to 64, adding dropout layers and enabling early stopping. Note that the rate in the Dropout layer is the percentage of connections between layers that are being removed. the SpatialDropout2D is a special kind of dropout layer for convolutional neural networks, which drops certain multiplications of the filter matrix with parts of the original image before pooling across all movements over the original image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ptjos\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Build a convolutional neural network with two convolutional layers\n",
    "# Decrease number of neurons and add dropout to reduce overfitting\n",
    "conv_model_reduce_overfit = Sequential()\n",
    "conv_model_reduce_overfit.add(Conv2D(64, (3, 3), input_shape = (28, 28, 1)))\n",
    "conv_model_reduce_overfit.add(Activation('relu'))\n",
    "conv_model_reduce_overfit.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model_reduce_overfit.add(Dropout(0.5))\n",
    "conv_model_reduce_overfit.add(Conv2D(64, (3, 3)))\n",
    "conv_model_reduce_overfit.add(SpatialDropout2D(0.5))\n",
    "conv_model_reduce_overfit.add(Activation('relu'))\n",
    "conv_model_reduce_overfit.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "conv_model_reduce_overfit.add(Flatten())\n",
    "conv_model_reduce_overfit.add(Dense(64))\n",
    "conv_model_reduce_overfit.add(Dropout(0.5))\n",
    "conv_model_reduce_overfit.add(Dense(10))\n",
    "conv_model_reduce_overfit.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting our model, we also enable early stopping to reduce overfitting. Instead of going through all epochs specified, early stopping automatically stops the iterations through the epoch once it’s being noticed that the validation loss increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      " - 81s - loss: 0.7721 - acc: 0.7258 - val_loss: 0.4541 - val_acc: 0.8372\n",
      "Epoch 2/10\n",
      " - 80s - loss: 0.5176 - acc: 0.8179 - val_loss: 0.3976 - val_acc: 0.8557\n",
      "Epoch 3/10\n",
      " - 80s - loss: 0.4587 - acc: 0.8387 - val_loss: 0.3620 - val_acc: 0.8674\n",
      "Epoch 4/10\n",
      " - 79s - loss: 0.4275 - acc: 0.8484 - val_loss: 0.3414 - val_acc: 0.8759\n",
      "Epoch 5/10\n",
      " - 79s - loss: 0.4074 - acc: 0.8572 - val_loss: 0.3250 - val_acc: 0.8837\n",
      "Epoch 6/10\n",
      " - 80s - loss: 0.3935 - acc: 0.8606 - val_loss: 0.3122 - val_acc: 0.8874\n",
      "Epoch 7/10\n",
      " - 79s - loss: 0.3836 - acc: 0.8650 - val_loss: 0.3030 - val_acc: 0.8889\n",
      "Epoch 8/10\n",
      " - 78s - loss: 0.3722 - acc: 0.8674 - val_loss: 0.3009 - val_acc: 0.8906\n",
      "Epoch 9/10\n",
      " - 77s - loss: 0.3678 - acc: 0.8707 - val_loss: 0.2890 - val_acc: 0.8958\n",
      "Epoch 10/10\n",
      " - 78s - loss: 0.3625 - acc: 0.8710 - val_loss: 0.2894 - val_acc: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b31958c9c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model with adam optimizer and accuracy metric\n",
    "# Categorical cross-entropy is the loss function for one-hot encoded labels and\n",
    "# batch size equal to the number of neurons in the convolutional layers and 10 epochs\n",
    "# Add early stopping to avoid overfitting\n",
    "conv_model_reduce_overfit.compile(loss = \"categorical_crossentropy\", \n",
    "                   optimizer = 'adam', metrics = ['accuracy'])\n",
    "conv_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "conv_model_reduce_overfit.fit(train_images, train_labels_bin, validation_split = 0.3,\n",
    "               epochs = 10, verbose = 2, callbacks = [conv_callback], batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 533us/step\n",
      "Convolutional model ten epochs reduced overfit -- Test loss: 30.907997370958327\n",
      "Convolutional model ten epochs reduced overfit -- Test accuracy: 88.84\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the test loss and accuracy\n",
    "test_loss, test_acc = conv_model_reduce_overfit.evaluate(test_images, test_labels_bin)\n",
    "print(\"Convolutional model ten epochs reduced overfit -- Test loss:\", test_loss * 100)\n",
    "print(\"Convolutional model ten epochs reduced overfit -- Test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we observe that although the training and test accuracies have decreased, they are now much more similar than before. The test accuracy has not decreased substantially, but the training accuracy has, which means that overfitting is much less of a problem than before. Next, we can print the first ten predictions from the model and the first ten actual labels and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0: 9\n",
      "Actual 0: 9\n",
      "Prediction 1: 2\n",
      "Actual 1: 2\n",
      "Prediction 2: 1\n",
      "Actual 2: 1\n",
      "Prediction 3: 1\n",
      "Actual 3: 1\n",
      "Prediction 4: 6\n",
      "Actual 4: 6\n",
      "Prediction 5: 1\n",
      "Actual 5: 1\n",
      "Prediction 6: 4\n",
      "Actual 6: 4\n",
      "Prediction 7: 6\n",
      "Actual 7: 6\n",
      "Prediction 8: 5\n",
      "Actual 8: 5\n",
      "Prediction 9: 7\n",
      "Actual 9: 7\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print predictions versus actual labels\n",
    "predictions = conv_model_reduce_overfit.predict(test_images)\n",
    "for i in range(10):\n",
    "  print(\"Prediction \" + str(i) + \": \" + str(np.argmax(np.round(predictions[i]))))\n",
    "  print(\"Actual \" + str(i) + \": \" + str(test_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these predictions to the first ten labels in the data set, we observe that the first ten predictions are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
